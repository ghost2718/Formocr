
opencv_python==4.1.0.25
torchvision==0.3.0

torch==1.1.0
Pillow==6.0.0
matplotlib==3.1.0
numpy==1.16.2
PyYAML==3.12


# Formocr
Form parsing project(For Internship at IITB)

# Pipeline:
The pipeline for the project consists of the following steps:</br>
## 1)Faster RCNN based object detection:</br>
First we pass the input image through faster RCNN model to detect the date and A/C no fields.</br>
The model was trained on 10 Images from the validation set provided.</br>
The model was technicaly overfit onto the 10 Images because we only need to learn 1 template and do not have the requirement for generalization over differnent templates.</br>
Overfitting the model onto one template provides significant accuracy in detection on that particular template at the cost of generalization accross templates.</br>
The model was trained using the luminoth library ([Luminoth](https://github.com/tryolabs/luminoth)) which is an interface to train and use object detection models.</br>
The model was trained for 550 epochs.</br>

The Trained model is used to get Reigons of Intrest which are then extracted and then passed onto the next part of the pipeline.</br>
#### Reigons of Intrest extracted:</br>















## 2)Charater Segmentation:</br>
Characters are segmented from the Reigions of Intrest(ROI) using traditional Computer vision and Image processing techniques.
Each ROI image is first converted to grayscale.</br>
The Grayscale Image is then Blurred using gaussian blur which is used to remove noise and smooth around edges.</br>
The Blurred Image is later binirized using Otsu thresholding.</br>
The we find countours which are later sorted.</br>
After recieving the coordinates of all the countours we extract all the ones which satisfy predefined parameters.</br>
These extracted countours are then saved in their respective directories.

#### Segmented Characters:














## 3)Prediction:</br>
The segmented characters are then passed through the clasifier model to obtain predictions
#### Classifier Model:</br>
The Classifier model is a simple convolutional neural network with 2 convolutional layers  and 2 fully connected layers but,it also includes a [Spatial Transformer Network Module](https://arxiv.org/abs/1506.02025).</br>
The primary Advantage of spatial transformer networks is that they make the model invariant to transformations in the input image by learning the transformations to apply to the input image which normailizes them.</br>
They also dont need any special kind of training ,they just learn from normal training procedure.</br>
The model was trained on the MNIST dataset for 20 epochs.
The optimiser used is SGD with a batch size of 64.</br>

##### Training Plot:














The model trained on the above methods is used to predict the classes of the image which are then stored for further processing.


## 4)Output:
The predicted classes are then preproced as required and the results are then printed on the screen.</br>
The results are also stored onto a CSV file which is available on the output directory.





# SETUP AND RUNNING:


